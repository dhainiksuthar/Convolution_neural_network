{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "HorseVsPeople.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PrBllAuNyDf"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLUZ_hpuN8Xm",
        "outputId": "4688299d-5f50-434e-d578-0f10a405114f"
      },
      "source": [
        "!wget http://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip\n",
        "!wget http://storage.googleapis.com/laurencemoroney-blog.appspot.com/validation-horse-or-human.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-22 14:47:58--  http://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.214.128, 172.253.119.128, 108.177.111.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.214.128|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 149574867 (143M) [application/zip]\n",
            "Saving to: ‘horse-or-human.zip’\n",
            "\n",
            "horse-or-human.zip  100%[===================>] 142.65M  89.1MB/s    in 1.6s    \n",
            "\n",
            "2021-10-22 14:48:00 (89.1 MB/s) - ‘horse-or-human.zip’ saved [149574867/149574867]\n",
            "\n",
            "--2021-10-22 14:48:00--  http://storage.googleapis.com/laurencemoroney-blog.appspot.com/validation-horse-or-human.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.198.128, 64.233.191.128, 173.194.74.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.198.128|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11480187 (11M) [application/zip]\n",
            "Saving to: ‘validation-horse-or-human.zip’\n",
            "\n",
            "validation-horse-or 100%[===================>]  10.95M  40.6MB/s    in 0.3s    \n",
            "\n",
            "2021-10-22 14:48:00 (40.6 MB/s) - ‘validation-horse-or-human.zip’ saved [11480187/11480187]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3zEKLooOB0R"
      },
      "source": [
        "TRAIN_DIR = \"./train_data\"\n",
        "VAL_DIR = \"./val_data\"\n",
        "!unzip -q horse-or-human.zip -d $TRAIN_DIR\n",
        "!unzip -q validation-horse-or-human.zip -d $VAL_DIR"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxkQ_5cEOoeW"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale = 1./255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range = 0.2,\n",
        "    height_shift_range = 0.2,\n",
        "    shear_range = 0.2,\n",
        "    fill_mode = 'nearest',\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        ")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUjSLQ5UPBrF"
      },
      "source": [
        "test_datagen = ImageDataGenerator(\n",
        "    rescale = 1./255,\n",
        ")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFHBrPjnPI1k",
        "outputId": "3da8304b-bd62-4661-ac1a-d15183c1a6df"
      },
      "source": [
        "train_data = train_datagen.flow_from_directory(\n",
        "    TRAIN_DIR,\n",
        "    batch_size = 64,\n",
        "    target_size = (176, 176),\n",
        "    class_mode = 'binary',\n",
        ")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1027 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aO8H1eUzPZBU",
        "outputId": "a2aa1d63-c425-4116-d8c4-34d20b2022cc"
      },
      "source": [
        "test_data = test_datagen.flow_from_directory(\n",
        "    VAL_DIR,\n",
        "    batch_size = 64,\n",
        "    target_size = (176, 176),\n",
        "    class_mode = 'binary',\n",
        ")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 256 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqKkEl9POUtH",
        "outputId": "882fbe74-a4d2-4239-93c7-843fbf283e43"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "                                    tf.keras.layers.Conv2D(32, (3, 3), input_shape = (176, 176, 3), activation='relu'),\n",
        "                                    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "                                    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "\n",
        "                                    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "                                    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "                                    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "\n",
        "                                    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "                                    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "                                    tf.keras.layers.Flatten(),\n",
        "                                      \n",
        "                                    tf.keras.layers.Dense(512, activation='relu'),\n",
        "                                    tf.keras.layers.Dropout(0.5),\n",
        "                                    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# model = tf.keras.models.Sequential([\n",
        "#     tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(176,176,3)),\n",
        "#     tf.keras.layers.MaxPooling2D(2,2),\n",
        "    \n",
        "#     tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "#     tf.keras.layers.MaxPooling2D(2,2),\n",
        "    \n",
        "#     tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "#     tf.keras.layers.MaxPooling2D(2,2),\n",
        "    \n",
        "#     tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "#     tf.keras.layers.MaxPooling2D(2,2),\n",
        "    \n",
        "#     tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "#     tf.keras.layers.MaxPooling2D(2,2),\n",
        "    \n",
        "#     # tf.keras.layers.Flatten(),\n",
        "#     tf.keras.layers.Dense(64, activation='relu'),\n",
        "#     tf.keras.layers.Dense(1, activation='sigmoid'),\n",
        "# ])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_10 (Conv2D)           (None, 174, 174, 32)      896       \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 172, 172, 32)      9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 86, 86, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 84, 84, 64)        18496     \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 82, 82, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 41, 41, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 39, 39, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling (None, 19, 19, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 23104)             0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 512)               11829760  \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 513       \n",
            "=================================================================\n",
            "Total params: 11,932,769\n",
            "Trainable params: 11,932,769\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GbafHE_QCxV"
      },
      "source": [
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9z8FE0CVQOBP",
        "outputId": "c735c865-834e-4e19-a447-f7da351554e1"
      },
      "source": [
        "history = model.fit(\n",
        "      train_data,\n",
        "      steps_per_epoch=8,  \n",
        "      epochs=15,\n",
        "      verbose=1,\n",
        "      validation_data = test_data,\n",
        "      validation_steps=8)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "8/8 [==============================] - ETA: 0s - loss: 2.0185 - acc: 0.4883WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 8 batches). You may need to use the repeat() function when building your dataset.\n",
            "8/8 [==============================] - 10s 1s/step - loss: 2.0185 - acc: 0.4883 - val_loss: 0.6905 - val_acc: 0.5000\n",
            "Epoch 2/15\n",
            "8/8 [==============================] - 7s 846ms/step - loss: 0.6823 - acc: 0.5664\n",
            "Epoch 3/15\n",
            "8/8 [==============================] - 6s 750ms/step - loss: 0.6817 - acc: 0.6208\n",
            "Epoch 4/15\n",
            "8/8 [==============================] - 6s 750ms/step - loss: 0.7744 - acc: 0.6341\n",
            "Epoch 5/15\n",
            "8/8 [==============================] - 7s 857ms/step - loss: 0.6014 - acc: 0.6914\n",
            "Epoch 6/15\n",
            "8/8 [==============================] - 7s 854ms/step - loss: 0.5835 - acc: 0.7266\n",
            "Epoch 7/15\n",
            "8/8 [==============================] - 7s 860ms/step - loss: 0.5207 - acc: 0.7305\n",
            "Epoch 8/15\n",
            "8/8 [==============================] - 6s 767ms/step - loss: 0.6790 - acc: 0.7051\n",
            "Epoch 9/15\n",
            "8/8 [==============================] - 6s 741ms/step - loss: 0.7518 - acc: 0.7472\n",
            "Epoch 10/15\n",
            "8/8 [==============================] - 7s 872ms/step - loss: 0.4920 - acc: 0.7539\n",
            "Epoch 11/15\n",
            "8/8 [==============================] - 7s 862ms/step - loss: 0.4381 - acc: 0.7969\n",
            "Epoch 12/15\n",
            "8/8 [==============================] - 6s 886ms/step - loss: 0.4998 - acc: 0.7805\n",
            "Epoch 13/15\n",
            "8/8 [==============================] - 7s 867ms/step - loss: 0.7413 - acc: 0.7188\n",
            "Epoch 14/15\n",
            "8/8 [==============================] - 6s 879ms/step - loss: 0.4479 - acc: 0.7805\n",
            "Epoch 15/15\n",
            "8/8 [==============================] - 7s 868ms/step - loss: 0.4279 - acc: 0.8125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyvEtQEqQX0J"
      },
      "source": [
        ""
      ],
      "execution_count": 10,
      "outputs": []
    }
  ]
}